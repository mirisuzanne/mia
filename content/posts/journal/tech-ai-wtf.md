---
title: Tech continues to be political
sub: And the politics aren't looking great
date: 2025-02-12
summary: |
  Being "in tech" in 2025 is depressing,
  and if I'm going to stick around,
  I need to remember why I'm here.
---

## You are here, on my personal web log

A note of warning,
before you proceed:
this is a journal entry,
at a difficult time.

> It’s very hard to think or act
> when you can’t tell if you’re about to lose your job,
> have your research killed off,
> have your healthcare terminated,
> witness unstoppable crimes,
> or just experience extended and
> apparently unescapable moral injury.
>
> ---Erin Kissane, [Against Entropy](https://erinkissane.com/against-entropy)

TL;DR -- This is a post about
billionaires who love eugenics,
support a pro-eugenics government,
and sell us a product
that they promise will help their longterm
eugenic goals.
But really this is a post about how I feel,
when colleagues treat that product
as though it _might have merit, if we just give it a chance_.

For some reason,
I find that opinion to be _in bad taste_.
I know I shouldn't yuck your yum, or whatever,
but I don't like eugenics.

Reader, _it fucks me up_.

## Chill out, it's just a tool

For years we've been saying that
_tech is political_,
and that _tech is not neutral_.
But I don't know if we're communicating
the full nuance of that adage.
It's not just a warning about bad Apples (or Palantirs)
who might use code to dabble in evil extracurriculars.
More important to me
is the understanding
that _technologies often carry an ideology inside them_:

> It is something of an amusing curiosity
> that some AI models were perplexed
> by a giraffe without spots.
> But it's these same tools and paradigms
> that enshrine normativity of all kinds,
> "sanding away the unusual."
>
> ---Ben Myers, [I'm a Spotless Giraffe](https://benmyers.dev/blog/spotless-giraffe/)

Tools tend to exist between us and a goal,
and the shape of the tool
tells us something about how to proceed,
and what outcomes are desirable.
Tech _enacts_ and _shapes_ our world,
our lives, and our politics.

Guns don't kill people,
guns are designed _to help people kill people_.

Maybe we should consider the beliefs and assumptions
that have been built into a technology
before we embrace it?
But we often prefer to treat each new toy
as as an abstract and unmotivated _opportunity_.
If only the _good people like ourselves_
would get involved early,
we can surely teach everyone else to use it ethically!

Every tool is a hammer,
with context lost to history --
and it's up to us
to determine _individually_
what looks like a nail.
There is no system, no society,
no marketing department, no regulation.
Each of us is an island of isolated ~~trolly conductors~~
_hammer enthusiasts_.

Once we've established some useful norms --
a 'best practice' or two --
I can't imagine anyone
[_crowd cheers for CEO giving sieg heil salute_].

## Meanwhile, back at the hammer factory…

The AI projects currently mid-hype
are being developed and sold by billionaires and VCs
with companies explicitly
pursuing surveillance, exploitation, and weaponry.
They fired their ethics teams
at the start of the cycle,
and diverted our attention
to a long-term sci-fi narrative
about the coming age of machines --
a "General Intelligence" that will soon
"surpasses" human ability.

_Be it god or demon,
only the high priests of venture capital
can summon and tame such a powerful being
for the good of humanity!
It will only cost you all your labor (past and present),
a reversal on climate policy,
and a rather large fortune._

What does that mean?
Hand-waving eugenics.
We have no way to measure _intelligence_,
no idea what it means to _surpass humans_,
and no reason to believe that 'intelligence'
might be exponential.
Unless you rely on debunked race science,
which many of these CEOs seem obsessed with.
Now they are eager to jump on board
an authoritarian movement
_that wants to exterminate trans and disabled people_,
fire black people,
and deport all my immigrant friends and colleagues.

Surely this has nothing to do with their products, though.

## But her use-cases

I know that 'AI' _broadly_ has a long history,
with 'language models' and 'neural nets'
developing real use-cases in science and other fields.
I'm not new here.
But this background level of validity-by-association
is used to prop up absolute garbage.
The chatSlop we're drowning in now
is clearly designed and deployed for a different purpose.

Haven't you heard?
They're building a digital god
who will lead us to salvation,
uploaded into the virgo supercluster
where we can expand the light of
[exponential profit](https://bengrosser.com/projects/andreessens-techno-optimist-manifesto-as-redaction-poetry//)
throughout the cosmos!
This is the actual narrative of several AI CEOs,
despite being easy to dismiss
as hyperbolic nonsense.
Why won't I focus on the actual use-cases?

_Why won't you focus
on the actual documented harms?
Somehow there is always room for people
to dismiss concerns as "overblown and unfounded"
past the first attempted coup,
and well into an authoritarian power grab._

But the bigger issue is that
they don't have to be _successful_ to be _dangerous_.
Because along the way,
these companies get to steal our work and sell it back to us,
lower our wages, de-skill our field,
bury us in slop, and mire us in algorithmic bureaucracy.
If the long-term space god thing doesn't work out,
at least they can make a profit in the short-term.

The beliefs of these CEOs aren't incidental
to the AI product they're selling us.
These are not tools designed for us to _benefit from_,
but tools designed to _exploit us_.
To poison our access to jobs,
and our access to information at the same time.

I said on social media
that people _believe_ what chatbots tell them,
and I was laughed at.
_No one would trust a chatbot, silly_!
That same day,
several different friends and colleagues
quoted the output of an 'AI' to me
in unrelated situations,
as though quoting reliable facts.

So now
a select few companies run by billionaires
control much of the information
that people see --
"summarized" without sources.
Meanwhile,
there's an oligarchy taking power in the US.
Meanwhile,
Grok's entire purpose is to be 'anti-woke' and anti-trans,
_[ChatGPT’s political views are shifting right](https://gizmodo.com/chatgpts-political-views-are-shifting-right-a-new-analysis-finds-2000562328)_,
and Anthropic is partnering with Palantir.

Seems chill.
I bet 'agents' are cool.

_Wouldn't want to eat
[a shrimp cocktail in the rain](https://defector.com/salesforce-is-using-a-hallucination-to-sell-ai)._

## Tech workers seem to like tech actually

[There's a meme](https://knowyourmeme.com/photos/2069350-twitter-x)
that goes around regularly,
about the attitudes of _tech enthusiasts_
vs _tech workers_…

> Tech enthusiasts: My entire house is smart.
>
> Tech workers: The only piece of technology
> in my house is a printer
> and I keep a gun next to it
> so I can shoot it
> if it makes a noise I don't recognize.
>
> ---[Pranay Pathole](https://knowyourmeme.com/photos/2069350-twitter-x)

I can relate to that sentiment,
but many in our community seem _unfazed_
or even _excited_ about 'AI' and 'agents'
and 'codegen' and all the rest of it.
As far as I can tell,
most of our industry is still on board with the project,
even while protesting the changes in corporate politics,
or occasionally complaining about the most obvious over-use.
There are certainly a number of people
raising alarms or expressing frustration,
but we're often dismissed as _uninformed_.

Based on every conference I've attended over the last year,
I can absolutely say we're a fringe minority.
And it's wearing me out.
I don't know how to participate
in a community that so eagerly brushes aside
the active and _intentional_/_foundational_ harms of a technology.
In return for what? Faster copypasta?
Automation tools being rebranded as an "agentic" web?
Assurance that we won't be _left behind_?

_This is your opportunity
to get in at the ground floor!_

I don't know how to attend conferences
full of gushing talks about the tools
that were _designed to negate me_.
That feels so absurd to say.
I don't have any interest in trying to
reverse-engineer use-cases for it,
or improve the flaws to make it "better",
or help sell it by bending it to new uses.

When eugenics-obsessed billionaires
try to sell me a new toy,
I don't ask how many keystrokes it will save me at work.
It's impossible for me to discuss the _utility_ of a thing
when I fundamentally disagree with the _purpose_ of it.

I don't care how well their 'AI' _works_ --
or if you found a fancy fun use-case.
It _fucks me up_ watching peers treat this tech
_from people who want to eradicate me_
as a future worth considering.
I don't want any of this.

I don't need _an agent_,
I want to _maintain my own agency_.

## I don't know

> I used to see the AI bubble
> and trans rights as distinct issues.
> I no longer do.
> The fascist movement in tech
> has truly metastasized,
> as evidenced by Elon Musk's personal coup,
> his endless supply of techbro supporters,
> tech companies' eagerness to axe DEI programs
> once Trump gave them an excuse,
> erasure of queer lives from tech products, etc.
>
> To the extent that AI marketing
> is an attempt to enclose and commodify culture,
> and thus to concentrate political power,
> I see it as a kind of fascism.
>
> ---[Cassandra Granade](https://wandering.shop/@xgranade/113987828759081498)

Every time I log on
I feel like I'm being gaslit --
asked to train my shitty replacement,
and then step aside.
The future is _not women_,
I'm learning now.
You can be sued in the US
for intentionally hiring women.
The future is actually _inhuman word synthesizers_.

_Oh no, I fell for the genders
with their sneaky ideology!
Now I'm a crime!
Haha, oops!_

Work is already harder to find,
and companies mostly want help
slopping more slop into the slop machine.
_Because it will help users_, you ask?
Of course not!
Jut because everyone has slop on-tap,
and needs to do somewhere to put it all!

~~_That's the trouble with tribbles._~~
[Money, gain, profit](https://business-business.business/)!

What are we doing here?
What am I doing here?
How do I stay engaged in this field,
and keep paying my bills,
without feeling like a constant outsider --
about to be dismissed from my career?
I know I'm not the only one feeling this way,
but the layering of threats and betrayals add up.
It feels so isolating.

## It's probably good to get this clarity

"Tech" was always a vague and hand-waving field --
a way to side-step regulations
while starting an unlicensed taxi company or hotel chain.
That was never my interest.

But I got curious about _the web_,
a weird little project
built for sharing research between scientists.
And I still think this web could be pretty cool, actually,
if it wasn't trapped in the clutches of big tech.
If we can focus on
the bits that make it special --
the bits that make it unwieldy for capitalism:

> Large companies find HTML & CSS frustrating “at scale”
> because the web is
> a fundamentally anti-capitalist mashup art experiment,
> designed to give consumers all the power.
>
> ---Me, [before all this](https://tweets.miriamsuzanne.com/1198717316269084672/)

What are we going to build now --
those of us who still care about
diversity, equity, inclusion, accessibility,
and giving consumers the power?
Can we still put our HTML & CSS to good use?
Can we get back to building a web
where people have _agency_ instead of _inhuman agents_?

Where are you looking to put your energy next?
